{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2811680,"sourceType":"datasetVersion","datasetId":1718172},{"sourceId":6234848,"sourceType":"datasetVersion","datasetId":3581652},{"sourceId":8033289,"sourceType":"datasetVersion","datasetId":4735249},{"sourceId":11585183,"sourceType":"datasetVersion","datasetId":7263966},{"sourceId":429251,"sourceType":"modelInstanceVersion","modelInstanceId":349890,"modelId":371143},{"sourceId":461005,"sourceType":"modelInstanceVersion","modelInstanceId":373177,"modelId":394024},{"sourceId":609906,"sourceType":"modelInstanceVersion","modelInstanceId":457977,"modelId":473897},{"sourceId":610903,"sourceType":"modelInstanceVersion","modelInstanceId":458827,"modelId":474723},{"sourceId":610908,"sourceType":"modelInstanceVersion","modelInstanceId":458831,"modelId":474726},{"sourceId":613828,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":461216,"modelId":476980},{"sourceId":614418,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":461692,"modelId":477452}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ===============================================","metadata":{}},{"cell_type":"code","source":"#already\n!pip install SimpleITK\n!pip install monai==1.4.0\n!pip install einops==0.8.1\n!pip install progress_table\n!pip install medpy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:04.651497Z","iopub.execute_input":"2025-10-20T14:28:04.652547Z","iopub.status.idle":"2025-10-20T14:28:46.057195Z","shell.execute_reply.started":"2025-10-20T14:28:04.652507Z","shell.execute_reply":"2025-10-20T14:28:46.056061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/input","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:46.059107Z","iopub.execute_input":"2025-10-20T14:28:46.059446Z","iopub.status.idle":"2025-10-20T14:28:46.066459Z","shell.execute_reply.started":"2025-10-20T14:28:46.059404Z","shell.execute_reply":"2025-10-20T14:28:46.065458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ===================================================================================================\n### Testing","metadata":{}},{"cell_type":"code","source":"import logging\nimport os\nfrom datetime import datetime\nimport wandb\nfrom tqdm import tqdm  # Import tqdm for progress bars\n#Lib for preprocess + train\nimport SimpleITK as sitk\nimport torch\nfrom sklearn.model_selection import KFold\nfrom torch.utils.data.dataset import Dataset\nimport argparse\nimport random\nimport pathlib\nimport time\nimport torch.backends.cudnn as cudnn\nimport torch.nn.parallel\nimport torch.optim\nimport torch.utils.data\nimport yaml\nfrom monai.data import decollate_batch\nimport math\nimport shutil\n\nimport os\nimport cv2\nfrom glob import glob\nimport numpy as np\nimport pandas as pd\nimport nibabel as nib\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n\n#lib for model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nfrom torch import optim\nimport math \nfrom torch.autograd import Variable\nimport einops\nfrom einops import rearrange, repeat \nfrom einops.layers.torch import Rearrange, Reduce\nfrom torch import nn, einsum\nfrom monai.networks.blocks.dynunet_block import get_conv_layer\nfrom monai.networks.blocks import UnetOutBlock\n#for training\nfrom functools import partial\nfrom monai.metrics import DiceMetric\nfrom monai.inferers import sliding_window_inference\nfrom monai.transforms import (\n    SpatialCrop,\n    Activations,\n    AsDiscrete,\n    Compose,\n    EnsureType\n)\n\nfrom monai.losses import DiceLoss #Minh co can code lai ham loss khong?\nimport monai.transforms as transforms\nfrom monai.transforms import NormalizeIntensity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:46.067718Z","iopub.execute_input":"2025-10-20T14:28:46.068048Z","iopub.status.idle":"2025-10-20T14:28:46.081878Z","shell.execute_reply.started":"2025-10-20T14:28:46.068010Z","shell.execute_reply":"2025-10-20T14:28:46.081032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#đáng lẽ hai cái này là hai cái khác nhau nhưng vì ở đây mình dùng bộ data đã có sẵn traningdata nên trùng\n\nBRATS_TRAIN_FOLDERS = \"/kaggle/input/brats2021/BraTS_2021_TrainingData/BraTS_2021_TrainingData\"\nDATA_PATH = \"/kaggle/input/brats2021/BraTS_2021_TrainingData/BraTS_2021_TrainingData\"\nVALI_PATH = '/kaggle/input/brats2021/BraTS_2021_ValidationData/BraTS_2021_ValidationData'\n\n\n\nDATA_TYPES = ['t1', 't1ce', 't2', 'flair', 'seg']\nMASK_LABELS = ['Not Tumor', 'Non-Enhancing Tumor Core', 'Peritumoral Edema', 'GD-Enhancing Tumor']\nMASK_VALUES = [0, 1, 2, 4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:46.083561Z","iopub.execute_input":"2025-10-20T14:28:46.083816Z","iopub.status.idle":"2025-10-20T14:28:46.094065Z","shell.execute_reply.started":"2025-10-20T14:28:46.083790Z","shell.execute_reply":"2025-10-20T14:28:46.093363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#preprocessing function\n\"\"\"functions to correctly pad or crop non uniform sized MRI (before batching in the dataloader).\n\"\"\"\ndef get_brats_folder(on=\"train\"):\n    if on == \"train\":\n        return BRATS_TRAIN_FOLDERS\n    else:\n        return VALI_PATH\ndef pad_or_crop_image(image, seg=None, target_size=(128, 144, 144)):\n    c, z, y, x = image.shape\n    z_slice, y_slice, x_slice = [get_crop_slice(target, dim) for target, dim in zip(target_size, (z, y, x))]\n    image = image[:, z_slice, y_slice, x_slice]\n    if seg is not None:\n        seg = seg[:, z_slice, y_slice, x_slice]\n    todos = [get_left_right_idx_should_pad(size, dim) for size, dim in zip(target_size, [z, y, x])]\n    padlist = [(0, 0)]  # channel dim\n    for to_pad in todos:\n        if to_pad[0]:\n            padlist.append((to_pad[1], to_pad[2]))\n        else:\n            padlist.append((0, 0))\n    image = np.pad(image, padlist)\n    if seg is not None:\n        seg = np.pad(seg, padlist)\n        return image, seg\n    return image\n\ndef get_left_right_idx_should_pad(target_size, dim):\n    if dim >= target_size:\n        return [False]\n    elif dim < target_size:\n        pad_extent = target_size - dim\n        left = random.randint(0, pad_extent)\n        right = pad_extent - left\n        return True, left, right\n\ndef get_crop_slice(target_size, dim):\n    if dim > target_size:\n        crop_extent = dim - target_size\n        left = random.randint(0, crop_extent)\n        right = crop_extent - left\n        return slice(left, dim - right)\n    elif dim <= target_size:\n        return slice(0, dim)\ndef get_crop_slice2(target_size, dim):\n    if dim > target_size:\n        start = (dim - target_size) // 2\n        end = start + target_size\n        return slice(start, end)\n    elif dim <= target_size:\n        return slice(0, dim)\ndef pad_or_crop_image_label(image, seg=None, target_size=(128, 128, 128)):\n    c, z, y, x = image.shape\n    z_slice, y_slice, x_slice = [get_crop_slice2(target, dim) for target, dim in zip(target_size, (z, y, x))]\n    image = image[:, z_slice, y_slice, x_slice]\n    if seg is not None:\n        seg = seg[:, z_slice, y_slice, x_slice]\n    \n    # Pad nếu cần\n    # todos = [get_left_right_idx_should_pad(size, dim) for size, dim in zip(target_size, [z, y, x])]\n    # padlist = [(0, 0)]  # channel dim\n    # for to_pad in todos:\n    #     if to_pad[0]:\n    #         padlist.append((to_pad[1], to_pad[2]))\n    #     else:\n    #         padlist.append((0, 0))\n    \n    # # Pad ảnh để đạt được target_size\n    # image = np.pad(image, padlist)\n    if seg is not None:\n        # seg = np.pad(seg, padlist)\n        return image, seg\n    return image\n\n\ndef normalize(image):\n    \"\"\"Basic min max scaler.\n    \"\"\"\n    min_ = np.min(image)\n    max_ = np.max(image)\n    scale = max_ - min_\n    image = (image - min_) / scale\n    return image\n\ndef irm_min_max_preprocess(image, low_perc=1, high_perc=99):\n    \"\"\"Main pre-processing function used for the challenge (seems to work the best).\n\n    Remove outliers voxels first, then min-max scale.\n\n    Warnings\n    --------\n    This will not do it channel wise!!\n    \"\"\"\n\n    non_zeros = image > 0\n    low, high = np.percentile(image[non_zeros], [low_perc, high_perc])\n    image = np.clip(image, low, high)\n    image = normalize(image)\n    return image\n\ndef zscore_normalise(img):\n    # Đảm bảo rằng img có kiểu dữ liệu float\n    img = img.astype(np.float32)  # Nếu img là ndarray\n    # img = img.to(torch.float32)  # Nếu img là tensor\n    \n    slices = slice(None)\n    img[slices] = (img[slices] - np.mean(img[slices])) / np.std(img[slices])\n    return img\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:46.095299Z","iopub.execute_input":"2025-10-20T14:28:46.095877Z","iopub.status.idle":"2025-10-20T14:28:46.113891Z","shell.execute_reply.started":"2025-10-20T14:28:46.095849Z","shell.execute_reply":"2025-10-20T14:28:46.113082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Brats(Dataset):\n    def __init__(self, patients_dir, benchmarking=False, training=True, data_aug=False,\n                 no_seg=False, normalisation=\"zscore\", normal=False):\n        super(Brats, self).__init__()\n        self.benchmarking = benchmarking\n        self.normalisation = normalisation\n        self.data_aug = data_aug\n        self.training = training\n        self.datas = []\n        self.validation = no_seg\n        self.normal = normal\n        self.patterns = [\".t1\", \".t1ce\", \".t2\", \".flair\"]\n        self.target_spacing = [1.0, 1.0, 1.0]\n        self.roi_size = [128, 128, 128]\n        if self.training:\n            self.transform = transforms.Compose([\n                transforms.CropForegroundd(\n                    keys=[\"image\", \"label\"],\n                    source_key=\"image\",\n                    k_divisible=self.roi_size,\n                ),\n                transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0), #aug\n                transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1), #aug\n                transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2), #aug\n                # transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n                transforms.RandScaleIntensityd(keys=\"image\", factors=0.5, prob=1.0), #aug\n                transforms.RandShiftIntensityd(keys=\"image\", offsets=0.5, prob=1.0), #aug\n            ])\n        else:\n            # Validation/Inference pipeline\n            self.transform = transforms.Compose([\n                transforms.CropForegroundd(\n                    keys=[\"image\", \"label\"],\n                    source_key=\"image\",\n                    k_divisible=self.roi_size,\n                )\n            ])\n        \n        if not no_seg:\n            self.patterns += [\".seg\"]\n        for patient_dir in patients_dir:\n            patient_id = patient_dir.name\n            # Construct paths for each modality\n            paths = [patient_dir / f\"{patient_id}{value}.nii\" for value in self.patterns]\n            \n            # Add patient data\n            patient = dict(\n                id=patient_id,\n                t1=paths[0], t1ce=paths[1], t2=paths[2], flair=paths[3],\n                seg=paths[4] if not no_seg else None\n            )\n            self.datas.append(patient)\n\n\n    \n        \n    def __getitem__(self, idx):\n        _patient = self.datas[idx]\n        \n        patient_image = {key: self.load_nii(_patient[key]) for key in _patient if key not in [\"id\", \"seg\"]} \n        #-> load t1 t1w t2 flair\n        if _patient[\"seg\"] is not None:\n            patient_label = self.load_nii(_patient[\"seg\"])\n            et = patient_label == 4 # ET (Enhancing Tumor) - label 4\n            et_present = 1 if np.sum(et) >= 1 else 0\n            tc = np.logical_or(patient_label == 4, patient_label == 1) #TC (Tumor Core) - label 1, 4\n            wt = np.logical_or(tc, patient_label == 2) # WT (Whole Tumor) label 1, 2, 4\n            patient_label = np.stack([et, tc, wt])\n        else:\n            patient_label = np.zeros(patient_image.shape)  # placeholders, not gonna use it\n            et_present = 0\n\n        if self.normal == False: \n            if self.normalisation == \"minmax\":\n                patient_image = {key: irm_min_max_preprocess(patient_image[key]) for key in patient_image}\n            elif self.normalisation == \"zscore\":\n                patient_image = {key: zscore_normalise(patient_image[key]) for key in patient_image}\n        else:\n            # patient_image = {key: zscore_normalise(patient_image[key]) for key in patient_image}\n            normalize_intensity = NormalizeIntensity(nonzero=True, channel_wise=True)\n            patient_image = {key: normalize_intensity(patient_image[key]) for key in patient_image}  # Chuẩn hóa cường độ cho mỗi ảnh\n\n        patient_image = np.stack([patient_image[key] for key in patient_image])\n\n        if self.training:\n            z_indexes, y_indexes, x_indexes = np.nonzero(np.sum(patient_image, axis=0) != 0)\n            zmin, ymin, xmin = [max(0, int(np.min(arr) - 1)) for arr in (z_indexes, y_indexes, x_indexes)]\n            zmax, ymax, xmax = [int(np.max(arr) + 1) for arr in (z_indexes, y_indexes, x_indexes)]\n            patient_image = patient_image[:, zmin:zmax, ymin:ymax, xmin:xmax]\n            patient_label = patient_label[:, zmin:zmax, ymin:ymax, xmin:xmax]              \n            patient_image, patient_label = pad_or_crop_image(patient_image, patient_label, target_size=(128, 128, 128))\n\n            if self.normal == True:\n                data_dict = {\n                    \"image\": patient_image,\n                    \"label\": patient_label,\n                }\n                transformed = self.transform(data_dict)\n    \n                patient_image = transformed[\"image\"]\n                patient_label = transformed[\"label\"]\n        else:\n            z_indexes, y_indexes, x_indexes = np.nonzero(np.sum(patient_image, axis=0) != 0)\n            zmin, ymin, xmin = [max(0, int(np.min(arr) - 1)) for arr in (z_indexes, y_indexes, x_indexes)]\n            zmax, ymax, xmax = [int(np.max(arr) + 1) for arr in (z_indexes, y_indexes, x_indexes)]\n            patient_image = patient_image[:, zmin:zmax, ymin:ymax, xmin:xmax]\n            patient_label = patient_label[:, zmin:zmax, ymin:ymax, xmin:xmax]  \n            patient_image, patient_label = pad_or_crop_image_label(patient_image, patient_label, target_size=(128, 128, 128))\n\n\n        # Tiến hành các bước tiếp theo với transformed\n        patient_image, patient_label = patient_image.astype(\"float16\"), patient_label.astype(\"bool\")\n        patient_image, patient_label = [torch.from_numpy(x) for x in [patient_image, patient_label]]\n\n        return dict(patient_id=_patient[\"id\"],\n                    image=patient_image, label=patient_label,\n                    seg_path=str(_patient[\"seg\"]) if self.training else str(_patient[\"t1\"]),\n                    crop_indexes=((zmin, zmax), (ymin, ymax), (xmin, xmax)),\n                    et_present=et_present,\n                    supervised=True,\n                    idx=idx,\n                    )\n\n    @staticmethod\n    def load_nii(path_folder):\n        return sitk.GetArrayFromImage(sitk.ReadImage(str(path_folder)))\n\n    def __len__(self):\n        return len(self.datas)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:46.115081Z","iopub.execute_input":"2025-10-20T14:28:46.115454Z","iopub.status.idle":"2025-10-20T14:28:46.135438Z","shell.execute_reply.started":"2025-10-20T14:28:46.115413Z","shell.execute_reply":"2025-10-20T14:28:46.134580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##### \n\"\"\"\nShould check this\n\nGet full data\n\"\"\"\n\n\ndef get_datasets(seed, on=\"train\", fold_number=0, normalisation=\"zscore\", use_fold = False):\n    base_folder_train = pathlib.Path(get_brats_folder(on)).resolve()\n    base_folder_vali = pathlib.Path(get_brats_folder(\"a\")).resolve()\n    assert base_folder_train.exists()\n    patients_dir_train = sorted([x for x in base_folder_train.iterdir() if x.is_dir()])\n    patients_dir_vali = sorted([x for x in base_folder_vali.iterdir() if x.is_dir()])\n\n    \n    if use_fold == True:\n        kfold = KFold(5, shuffle=True, random_state=seed)\n    \n        splits = list(kfold.split(patients_dir_train))\n        train_idx, test_idx = splits[fold_number]\n    \n    \n        train = [patients_dir_train[i] for i in train_idx]\n        test = [patients_dir_train[i] for i in test_idx]\n        train_dataset = Brats(train, training=True, normalisation=normalisation, normal = True)\n        test_dataset = Brats(test, training=False, benchmarking=True, normalisation=normalisation, normal = True)\n    else:\n        train_dataset = Brats(patients_dir_train, training=True, normalisation=normalisation, normal = True)\n        test_dataset = Brats(patients_dir_vali, training=False, benchmarking=True, normalisation=normalisation, normal = True)\n        \n    \n    return train_dataset, test_dataset\n\nfull_train_dataset, val_dataset = get_datasets(123, fold_number=0)\nprint(len(full_train_dataset), len(val_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:46.136360Z","iopub.execute_input":"2025-10-20T14:28:46.136629Z","iopub.status.idle":"2025-10-20T14:28:47.001272Z","shell.execute_reply.started":"2025-10-20T14:28:46.136604Z","shell.execute_reply":"2025-10-20T14:28:47.000416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nCreate loader for neural network\n\"\"\"\n\n\ntrain_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=1, shuffle=True,\n                                           num_workers=2, pin_memory=True, drop_last=True)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False,\n                                         pin_memory=True, num_workers=2)\n\nprint(\"Train dataset number of batch:\", len(train_loader))\nprint(\"Val dataset number of batch:\", len(val_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:47.002411Z","iopub.execute_input":"2025-10-20T14:28:47.002707Z","iopub.status.idle":"2025-10-20T14:28:47.010675Z","shell.execute_reply.started":"2025-10-20T14:28:47.002679Z","shell.execute_reply":"2025-10-20T14:28:47.009759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nTest val_load and train_loader\n\"\"\"\n\nfor batch in val_loader:\n    # Assuming your input data is a 4D tensor (batch_size, channels, height, width)\n    \n    data_shape = batch['image'].shape\n    label_shape = batch['label'].shape\n    print(\"Data vali shape in the first batch:\", data_shape)\n    print(\"Label vali shape in the first batch:\", label_shape)\n    break  # Print only the first batch\nfor batch in train_loader:\n    # Assuming your input data is a 4D tensor (batch_size, channels, height, width)\n    \n    data_shape = batch['image'].shape\n    label_shape = batch['label'].shape\n    print(\"Data train shape in the first batch:\", data_shape)\n    print(\"Label train shape in the first batch:\", label_shape)\n    break  # Print only the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:47.011864Z","iopub.execute_input":"2025-10-20T14:28:47.012098Z","iopub.status.idle":"2025-10-20T14:28:53.569574Z","shell.execute_reply.started":"2025-10-20T14:28:47.012074Z","shell.execute_reply":"2025-10-20T14:28:53.568084Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Set up model","metadata":{}},{"cell_type":"code","source":"class ProjectExciteLayer(nn.Module):\n    \"\"\"\n        Project & Excite Module, specifically designed for 3D inputs\n        *quote*\n    \"\"\"\n\n    def __init__(self, num_channels, reduction_ratio=4):\n        \"\"\"\n        :param num_channels: No of input channels\n        :param reduction_ratio: By how much should the num_channels should be reduced\n        \"\"\"\n        super(ProjectExciteLayer, self).__init__()\n        num_channels_reduced = num_channels // reduction_ratio\n        self.reduction_ratio = reduction_ratio\n        self.relu = nn.GELU()\n        self.conv_c = nn.Conv3d(in_channels=num_channels, out_channels=num_channels_reduced, kernel_size=1, stride=1)\n        self.conv_cT = nn.Conv3d(in_channels=num_channels_reduced, out_channels=num_channels, kernel_size=1, stride=1)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_tensor):\n        \"\"\"\n        :param input_tensor: X, shape = (batch_size, num_channels, D, H, W)\n        :return: output tensor\n        \"\"\"\n        batch_size, num_channels, D, H, W = input_tensor.size()\n\n        # Project:\n        # Average along channels and different axes\n        squeeze_tensor_w = F.adaptive_avg_pool3d(input_tensor, (1, 1, W))\n\n        squeeze_tensor_h = F.adaptive_avg_pool3d(input_tensor, (1, H, 1))\n\n        squeeze_tensor_d = F.adaptive_avg_pool3d(input_tensor, (D, 1, 1))\n\n        # tile tensors to original size and add:\n        final_squeeze_tensor = sum([squeeze_tensor_w.view(batch_size, num_channels, 1, 1, W),\n                                    squeeze_tensor_h.view(batch_size, num_channels, 1, H, 1),\n                                    squeeze_tensor_d.view(batch_size, num_channels, D, 1, 1)])\n\n        # Excitation:\n        final_squeeze_tensor = self.sigmoid(self.conv_cT(self.relu(self.conv_c(final_squeeze_tensor))))\n        output_tensor = torch.mul(input_tensor, final_squeeze_tensor)\n\n        return output_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.600377Z","iopub.execute_input":"2025-10-20T14:28:53.600763Z","iopub.status.idle":"2025-10-20T14:28:53.615765Z","shell.execute_reply.started":"2025-10-20T14:28:53.600722Z","shell.execute_reply":"2025-10-20T14:28:53.614601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DoubleConv(nn.Module):\n    \"\"\"(Conv3D -> BN -> ReLU) * 2\"\"\"\n    def __init__(self, in_channels, out_channels, num_groups=16, kernel_size=3, padding=1, stride=1, bias=True):\n        super().__init__()\n        self.double_conv = nn.Sequential(\n            nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias),\n            #nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n            nn.InstanceNorm3d(out_channels, affine = True),\n            nn.LeakyReLU(),\n            nn.Conv3d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride, bias=bias),\n            nn.InstanceNorm3d(out_channels, affine = True),\n            #nn.GroupNorm(num_groups=num_groups, num_channels=out_channels),\n            nn.LeakyReLU()\n        )\n    \n    def forward(self, x):\n        return self.double_conv(x)\nclass Downsampling(nn.Module):\n    def __init__(self, in_channels, out_channels, \n        kernel_size, stride=1, padding=0, \n        norm=None):\n        super().__init__()\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=kernel_size, \n                              stride=stride, padding=padding)\n        \n        self.norm = nn.InstanceNorm3d(out_channels, affine = True)#nn.GroupNorm(num_groups=8, num_channels=out_channels)\n\n    def forward(self, x): #downsample(conv) --> post norm\n        x = self.conv(x)\n        x = self.norm(x)\n        return x   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.726945Z","iopub.execute_input":"2025-10-20T14:28:53.727309Z","iopub.status.idle":"2025-10-20T14:28:53.739470Z","shell.execute_reply.started":"2025-10-20T14:28:53.727272Z","shell.execute_reply":"2025-10-20T14:28:53.738667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def MBConv(\n    dim_in,\n    dim_out,\n    *,\n    expansion_rate = 2,\n):\n    hidden_dim = int(expansion_rate * dim_out)\n    stride = 1\n\n    net = nn.Sequential(\n        nn.Conv3d(dim_in, hidden_dim, 1),\n        #nn.GroupNorm(num_groups=8, num_channels=hidden_dim),\n        nn.InstanceNorm3d(hidden_dim, affine = True),\n        nn.GELU(),\n        nn.Conv3d(hidden_dim, hidden_dim, 3, stride = stride, padding = 1, groups = hidden_dim),\n        #nn.GroupNorm(num_groups=8, num_channels=hidden_dim),\n        nn.InstanceNorm3d(hidden_dim, affine = True),\n        nn.GELU(),\n        ProjectExciteLayer(hidden_dim),\n        nn.Conv3d(hidden_dim, dim_out, 1),\n        #nn.GroupNorm(num_groups=8, num_channels=dim_out)\n        nn.InstanceNorm3d(dim_out, affine = True),\n    )\n    return net\n\nclass GLUMBConv(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size=3,\n        stride=1,\n        expand_ratio=2\n    ):\n        super().__init__()\n\n        mid_channels = round(in_channels * expand_ratio)\n\n        self.act = nn.SiLU()\n        self.norm = nn.InstanceNorm3d(out_channels, affine = True)#nn.GroupNorm(num_groups=8, num_channels=out_channels)\n        self.Leaky = nn.LeakyReLU()\n        self.inverted_conv = nn.Conv3d(in_channels,\n                                       mid_channels,\n                                       1,\n                                       padding = 0,\n                                       bias=True)\n        self.depth_conv = nn.Conv3d(\n            mid_channels,\n            mid_channels,\n            kernel_size,\n            stride=stride,\n            padding=1,\n            groups=mid_channels,\n            bias=True,\n        )\n        self.point_conv = nn.Conv3d(\n            mid_channels//2,\n            out_channels,\n            1,\n            padding = 0,\n            bias=False\n        )\n        self.Gnorm = nn.InstanceNorm3d(mid_channels, affine = True)#nn.GroupNorm(num_groups=8, num_channels=mid_channels)\n        self.PE = ProjectExciteLayer(num_channels = mid_channels//2)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.inverted_conv(x)\n        x = self.Gnorm(x)\n        x = self.Leaky(x)\n        \n        x = self.depth_conv(x)\n        x = self.Gnorm(x)\n        x = self.Leaky(x)\n        x, gate = torch.chunk(x, 2, dim=1)\n        gate = self.act(gate)\n        x = x * gate\n        x = self.PE(x)\n        x = self.point_conv(x)\n        x = self.norm(x)\n        return x\nclass PreNormResidual(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4, dropout = 0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.net = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.LeakyReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)\n\nclass PreNormResidual2(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.InstanceNorm3d(dim, affine = True)#nn.GroupNorm(num_groups=8, num_channels=dim)\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(self.norm(x)) + x\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 3, dropout = 0.):\n        super().__init__()\n        inner_dim = int(dim * mult)\n        self.net = nn.Sequential(\n            nn.Linear(dim, inner_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(inner_dim, dim),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x):\n        return self.net(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.740525Z","iopub.execute_input":"2025-10-20T14:28:53.740822Z","iopub.status.idle":"2025-10-20T14:28:53.756455Z","shell.execute_reply.started":"2025-10-20T14:28:53.740787Z","shell.execute_reply":"2025-10-20T14:28:53.755661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head = 32,\n        dropout = 0.,\n        window_size = (7,7,7)\n    ):\n        super().__init__()\n        assert (dim % dim_head) == 0, 'dimension should be divisible by dimension per head'\n\n        self.heads = dim // dim_head\n        self.scale = dim_head ** -0.5\n        self.to_qkv = nn.Linear(dim, dim * 3, bias = False)\n        self.attend = nn.Sequential(\n            nn.Softmax(dim = -1),\n            nn.Dropout(dropout)\n        )\n        self.to_out = nn.Sequential(\n            nn.Linear(dim, dim, bias = False),\n            nn.Dropout(dropout)\n        )\n        # relative positional bias\n        w1,w2,w3 = window_size\n        # 初始化相对位置索引矩阵[2*H-1,2*W-1,2*D-1,num_heads]\n        self.rel_pos_bias = nn.Embedding((2 * w1 - 1) *(2 * w2 - 1)*(2 * w3 - 1), self.heads)\n        pos1 = torch.arange(w1)\n        pos2 = torch.arange(w2)\n        pos3 = torch.arange(w3)\n        # 首先我们利用torch.arange和torch.meshgrid函数生成对应的坐标，[3,H,W,D] 然后堆叠起来，展开为一个二维向量，得到的是绝对位置索引。\n        grid = torch.stack(torch.meshgrid(pos1, pos2, pos3, indexing = 'ij'))\n        grid = rearrange(grid, 'c i j k -> (i j k) c')\n        # 广播机制，分别在第一维，第二维，插入一个维度，进行广播相减，得到 3, whd*ww, whd*ww的张量\n        rel_pos = rearrange(grid, 'i ... -> i 1 ...') - rearrange(grid, 'j ... -> 1 j ...') \n        rel_pos[...,0] += w1 - 1\n        rel_pos[...,1] += w2 - 1\n        rel_pos[...,2] += w3 - 1\n        # 做了个乘法操作，以进行区分,最后一维上进行求和，展开成一个一维坐标   a*x1 + b*x2 + c*x3  (a= hd b=d c =1) \n        rel_pos_indices = (rel_pos * torch.tensor([(2 *w2 - 1)*(2 *w3 - 1), (2 *w3 - 1), 1])).sum(dim = -1)\n        # 注册为一个不参与网络学习的变量\n        self.register_buffer('rel_pos_indices', rel_pos_indices, persistent = False)\n               \n\n    def forward(self, x):\n        batch, height, width, depth, window_height, window_width, window_depth ,_, device, h = *x.shape, x.device, self.heads\n        # flatten\n        x = rearrange(x, 'b x y z w1 w2 w3 d -> (b x y z) (w1 w2 w3) d')\n        # project for queries, keys, values\n        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n        # split heads\n        q, k, v = map(lambda t: rearrange(t, 'b n (h d ) -> b h n d', h = h), (q, k, v))\n        # scale\n        q = q * self.scale\n        # sim\n        sim = einsum('b h i d, b h j d -> b h i j', q, k)\n        # add positional bias\n        bias = self.rel_pos_bias(self.rel_pos_indices)\n        sim = sim + rearrange(bias, 'i j h -> h i j')\n        # attention\n        attn = self.attend(sim)\n        # aggregate\n        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n        # merge heads\n        out = rearrange(out, 'b h (w1 w2 w3) d -> b w1 w2 w3 (h d)', w1 = window_height, w2 = window_width, w3 = window_depth)\n        # combine heads out\n        out = self.to_out(out)\n        return rearrange(out, '(b x y z) ... -> b x y z ...', x = height, y = width, z = depth)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.757644Z","iopub.execute_input":"2025-10-20T14:28:53.757939Z","iopub.status.idle":"2025-10-20T14:28:53.772156Z","shell.execute_reply.started":"2025-10-20T14:28:53.757912Z","shell.execute_reply":"2025-10-20T14:28:53.771298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MaxViT_Block(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim = 512,\n        dim_head = 32,\n        window_size = (8,8,8),\n        dropout = 0.1,\n    ):\n        super().__init__()\n        w1,w2,w3 = window_size\n\n        self.net = nn.Sequential(\n            MBConv(dim, dim), # 1, 1, 1, 1, 8, 8, 8, 256\n            Rearrange('b d (x w1) (y w2) (z w3) -> b x y z w1 w2 w3 d', w1 = w1, w2 = w2, w3 = w3),  # block-like attention -> [2, 1, 1, 1, 8, 8, 8, 256]\n            PreNormResidual(dim, Attention(dim = dim, dim_head = dim_head, dropout = dropout, window_size = window_size)), #2, 1, 1, 1, 8, 8, 8, 256]\n            PreNormResidual(dim, FeedForward(dim = dim, dropout = dropout)),\n            Rearrange('b x y z w1 w2 w3 d -> b d (x w1) (y w2) (z w3)'),\n            \n            Rearrange('b d (x w1) (y w2) (z w3) -> b x y z w1 w2 w3 d', w1 = w1, w2 = w2, w3 = w3),  # block-like attention -> [2, 1, 1, 1, 8, 8, 8, 256]\n            PreNormResidual(dim, Attention(dim = dim, dim_head = dim_head, dropout = dropout, window_size = window_size)), #2, 1, 1, 1, 8, 8, 8, 256]\n            PreNormResidual(dim, FeedForward(dim = dim, dropout = dropout)),\n            Rearrange('b x y z w1 w2 w3 d -> b d (x w1) (y w2) (z w3)'),\n            )\n    def forward(self, x):\n        x = self.net(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.787546Z","iopub.execute_input":"2025-10-20T14:28:53.788255Z","iopub.status.idle":"2025-10-20T14:28:53.795219Z","shell.execute_reply.started":"2025-10-20T14:28:53.788214Z","shell.execute_reply":"2025-10-20T14:28:53.794531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Decoder1(nn.Module):\n    def __init__(self, in_channels, out_channels, upsample_kernel_size, lka_size, kernel_size = 3, spatial_dims=3, test = True, i = 0):\n        super().__init__()        \n\n        upsample_stride = upsample_kernel_size\n        self.upsample = nn.Upsample(scale_factor=upsample_kernel_size, mode='trilinear', align_corners=True)\n        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.Gnorm = nn.InstanceNorm3d(out_channels, affine = True)#nn.GroupNorm(num_groups=8, num_channels=out_channels)\n\n        self.conv_block = nn.Conv3d(out_channels + out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n\n        self.point_wise_conv = nn.Sequential(\n            nn.Conv3d(out_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n            nn.InstanceNorm3d(out_channels, affine = True),\n            #nn.GroupNorm(num_groups=8, num_channels=out_channels)\n        )\n\n        self.depth_wise_conv = nn.Sequential(\n            nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, groups=out_channels),\n            nn.InstanceNorm3d(out_channels, affine = True),\n            #nn.GroupNorm(num_groups=8, num_channels=out_channels),\n            nn.LeakyReLU(negative_slope=0.01 )\n        )\n        self.conv_2 = nn.Conv3d(in_channels=1, out_channels=2, kernel_size=1)\n        self.sigmoid = nn.Sigmoid() \n        self.Leaky = nn.LeakyReLU(negative_slope=0.01)\n        self.PE = ProjectExciteLayer(num_channels = out_channels)\n    def forward(self, x1, x2): #x2 = the skip connection\n        skip = x2\n        up1 = self.upsample(x1) #same as skip\n        up1 = self.conv(up1)\n        up1 = self.Gnorm(up1)\n        x2 = self.depth_wise_conv(x2)\n        x2 = torch.amax(x2, dim=1, keepdim=True) # 2, 1, 16, 16, 16   \n        x1 = torch.amax(up1, dim=1, keepdim=True) # 2, 1, 16, 16, 16\n        x1 = x1 + x2 #2 1 16 16 16 bang elementwise addition\n        x1 = self.conv_2(x1) #-> 2 2 16 16 16\n        x1 = F.softmax(x1, dim = 1) # 2 2 16 16 16\n        x1,x2 = x1.split(1, dim = 1) #each = 2 1 16 16 16\n        skip_channels = skip.size(1)\n        x2 = x2.repeat(1,skip_channels,1,1,1)*skip # 2 128 16 16 16\n        x2 = x2 + skip\n        x1 = x1.repeat(1,skip_channels,1,1,1)*up1 + up1\n\n        x1 = x1 * self.sigmoid(x2)\n        x2 = x2 * self.sigmoid(x1)\n       \n\n        x1 = x1+x2 #co the thu cac phep toan khac o day\n        x1 = self.point_wise_conv(x1) #out\n        x1 = self.sigmoid(x1)\n        x1 = skip * x1\n        x1 = self.point_wise_conv(x1)#out\n        x1 = self.PE(x1)\n        x1 = self.sigmoid(x1)\n        x1 = up1 * x1\n        out = torch.cat((x1, skip), dim=1) #channel_x1 + channel_up1\n        out = self.conv_block(out) #out_channel here \n        out = self.Leaky(out)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.810938Z","iopub.execute_input":"2025-10-20T14:28:53.811541Z","iopub.status.idle":"2025-10-20T14:28:53.826683Z","shell.execute_reply.started":"2025-10-20T14:28:53.811502Z","shell.execute_reply":"2025-10-20T14:28:53.826007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     # Create a random input tensor (Batch Size = 2, Channels = 256, H = 32, W = 32, D = 32)\n#     img = torch.randn(2, 64, 32, 32, 32).to(device)\n#     skip = torch.randn(2, 32, 64, 64, 64).to(device)\n\n#     # Initialize the EfficientAttention block\n#     efficient_attention = Up1(4*16, 2*16, upsample_kernel_size=2).to(device)#EfficientAttention(dim=32).to(device)\n#     # efficient_attention = EfficientLinearAttention(dim=64).to(device).to(device)#EfficientAttention(dim=32).to(device)\n\n#     # Run the forward pass to check if everything works\n#     output = efficient_attention(img,skip)\n\n#     # Print the output shape to verify\n#     print(f\"Output shape: {output.shape}\")\n#     total_params = sum(p.numel() for p in efficient_attention.parameters())\n#     print(f\"Total number of parameters: {total_params}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.827858Z","iopub.execute_input":"2025-10-20T14:28:53.828186Z","iopub.status.idle":"2025-10-20T14:28:53.840742Z","shell.execute_reply.started":"2025-10-20T14:28:53.828149Z","shell.execute_reply":"2025-10-20T14:28:53.839947Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Up2(nn.Module):\n    def __init__(self, in_channels, out_channels, trilinear=False, test = True):\n        super().__init__()\n        spatial_dims = 3\n        #self.upT = nn.ConvTranspose3d(in_channels, in_channels, kernel_size=2, stride=2)\n        self.upsample = nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True)\n        self.conv = nn.Conv3d(in_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=False)\n        self.convLK_in = nn.Sequential(\n            nn.InstanceNorm3d(in_channels, affine = True),\n            #nn.GroupNorm(num_groups=8, num_channels=in_channels),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Conv3d(in_channels, 2*out_channels, kernel_size=3, padding=1, stride=1, bias=False),\n        )\n        self.convLK_out = nn.Sequential(\n            nn.InstanceNorm3d(2*out_channels, affine = True),#nn.GroupNorm(num_groups=8, num_channels=2*out_channels),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Conv3d(2*out_channels, out_channels, kernel_size=3, padding=1, stride=1, bias=False)\n        )\n\n        #self.channel_wise_maxpool = torch.amax(input_tensor, dim=1, keepdim=True)\n    def forward(self, x1, x2): #x2 = the skip connection\n        x1 = self.conv(self.upsample(x1))\n        x1 = self.convLK_in(x1)#16 128 128 128\n        x1 = self.convLK_out(x1)\n        return x1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.845515Z","iopub.execute_input":"2025-10-20T14:28:53.845828Z","iopub.status.idle":"2025-10-20T14:28:53.854332Z","shell.execute_reply.started":"2025-10-20T14:28:53.845804Z","shell.execute_reply":"2025-10-20T14:28:53.853441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Downsampling + ProjectExciteLayer + LayerNormGeneral + LKA\nclass Encoder(nn.Module):\n    def __init__(self, in_channels, out_channels, lka_size):\n        super().__init__()\n        self.loop = lka_size\n        self.encode1 = Downsampling(in_channels, out_channels, \n                                    kernel_size=3, stride=2, padding=1)\n        self.MaxViT_Block = MaxViT_Block(dim = out_channels, #128                        # dimension of first layer, doubles every layer\n                                         dim_head = 16,                    # dimension of attention heads, kept at 32 in paper\n                                         window_size = (4,4,4),            # window size for block and grids 8 8 8 = out | 4 4 4 -> khong out\n        )\n    def forward(self, x):\n        x = self.encode1(x)\n        if self.loop == 21:\n            x1 = self.MaxViT_Block(x)\n        elif self.loop == 15:\n            x1 = self.MaxViT_Block(x)\n            # x1 = self.MaxViT_Block(x1)\n        else:\n            x1 = self.MaxViT_Block(x)\n            x1 = self.MaxViT_Block(x1)\n            # x1 = self.MaxViT_Block(x1)\n        skip = x1\n        return x1, skip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.855289Z","iopub.execute_input":"2025-10-20T14:28:53.855652Z","iopub.status.idle":"2025-10-20T14:28:53.867293Z","shell.execute_reply.started":"2025-10-20T14:28:53.855609Z","shell.execute_reply":"2025-10-20T14:28:53.866433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Bneck(nn.Module):\n    def __init__(self, in_channels, out_channels, trilinear=False, test = True):\n        super().__init__()\n        self.encode1 = Downsampling(in_channels, out_channels,kernel_size=3, stride=2, padding=1)\n        self.net = nn.Sequential(\n            nn.Conv3d(out_channels, out_channels, 1),\n            #nn.GroupNorm(num_groups=8, num_channels=out_channels),\n            nn.InstanceNorm3d(out_channels, affine = True),\n            nn.GELU(),\n            nn.Conv3d(out_channels, out_channels, 3, stride = 1, padding = 1, groups = out_channels),\n            nn.InstanceNorm3d(out_channels, affine = True),\n            #nn.GroupNorm(num_groups=8, num_channels=out_channels),\n            nn.GELU(),\n        )\n\n    def forward(self, x1): #x2 = the skip connection\n        x1 = self.encode1(x1)\n        x1 = self.net(x1)\n        return x1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.868329Z","iopub.execute_input":"2025-10-20T14:28:53.868676Z","iopub.status.idle":"2025-10-20T14:28:53.879607Z","shell.execute_reply.started":"2025-10-20T14:28:53.868632Z","shell.execute_reply":"2025-10-20T14:28:53.878855Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Unet(nn.Module):\n    def __init__(self, in_channels, n_channels, n_classes): \n        super().__init__()\n        self.in_channels = in_channels #4\n        self.n_classes = n_classes \n        self.n_channels = n_channels\n\n\n        self.conv = DoubleConv(in_channels, 2*n_channels, num_groups=8)\n        self.enc1 = Encoder(2*n_channels, 4*n_channels, lka_size = 21) #64\n        self.enc2 = Encoder(4*n_channels,  8*n_channels, lka_size = 15) #128\n        self.enc3 = Encoder(8*n_channels, 16*n_channels, lka_size = 10) #256\n\n        self.bottleneck = Bneck(16*n_channels, 32*n_channels) #512\n\n        \n        self.dec1 = Decoder1(32*n_channels, 16*n_channels, upsample_kernel_size=2, lka_size = 10) #concat(256|8 -> 128|16 (u1),skip) Out: 256|16 \n        self.dec2 = Decoder1(16*n_channels, 8*n_channels, upsample_kernel_size=2, lka_size = 15)  #concat(128|16 -> 64|32 (u2),skip) Out: 128|32\n        self.dec3 = Decoder1(8*n_channels, 4*n_channels, upsample_kernel_size=2, lka_size = 21)   #concat(64|32 -> 32|64 (u3),skip) Out: 64|64\n        \n        \n        self.dec4 = Up2(4*n_channels, n_channels)    #32|64 -> 16|128 (u4)\n        self.out = nn.Conv3d(in_channels=n_channels, out_channels=n_classes, kernel_size=1)\n        self.apply(self.initialize_weights)\n    def forward(self, x):\n        x = self.conv(x)\n        #---layer 1\n        x_1,skip_conv1 = self.enc1(x) #Embedding\n        #---layer 2\n        x_1,skip_conv2 = self.enc2(x_1)\n        #---layer 3 \n        x_1, skip_conv3 = self.enc3(x_1) \n        \n        #---bottleneck \n        x_1 = self.bottleneck(x_1)\n\n        #---layer 3 \n        x_out = self.dec1(x_1, skip_conv3) \n        #---layer 2 \n        x_out = self.dec2(x_out, skip_conv2) \n        #---layer 1 \n        x_out = self.dec3(x_out, skip_conv1) \n        #---layer output\n        x_out = self.dec4(x_out,x) # 16|128 (u4)\n        out = self.out(x_out) #3|128\n        return out\n    def initialize_weights(self, module):\n            name = module.__class__.__name__.lower()\n            if name in [\"conv2d\", \"conv3d\"]:\n                nn.init.kaiming_normal_(module.weight)\n            if hasattr(module, \"bias\") and module.bias is not None:\n                nn.init.constant_(module.bias, 0)\n\n\n\nmodel = Unet(in_channels=4, n_classes=3, n_channels=16).to('cuda')\nprint('Number of network parameters:', sum(param.numel() for param in model.parameters()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:53.880548Z","iopub.execute_input":"2025-10-20T14:28:53.880833Z","iopub.status.idle":"2025-10-20T14:28:54.040015Z","shell.execute_reply.started":"2025-10-20T14:28:53.880802Z","shell.execute_reply":"2025-10-20T14:28:54.039207Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## For the loss\n\n","metadata":{}},{"cell_type":"code","source":"\nclass EDiceLoss(nn.Module):\n    \"\"\"Dice loss tailored to Brats need.\n    \"\"\"\n\n    def __init__(self, do_sigmoid=True):\n        super(EDiceLoss, self).__init__()\n        self.do_sigmoid = do_sigmoid\n        self.labels = [\"ET\", \"TC\", \"WT\"]\n        self.device = \"cpu\"\n\n    def binary_dice(self, inputs, targets, label_index, metric_mode=False):\n        smooth = 1e-5\n        if self.do_sigmoid:\n            inputs = torch.sigmoid(inputs)\n\n        if metric_mode:\n            inputs = inputs > 0.5\n            if targets.sum() == 0:\n                print(f\"No {self.labels[label_index]} for this patient\")\n                if inputs.sum() == 0:\n                    return torch.tensor(1., device=\"cuda\")\n                else:\n                    return torch.tensor(0., device=\"cuda\")\n            # Threshold the pred\n        intersection = EDiceLoss.compute_intersection(inputs, targets)\n        if metric_mode:\n            dice = (2 * intersection) / ((inputs.sum() + targets.sum()) * 1.0)\n        else:\n            dice = (2 * intersection + smooth) / (inputs.pow(2).sum() + targets.pow(2).sum() + smooth)\n        if metric_mode:\n            return dice\n        return 1 - dice\n\n    @staticmethod\n    def compute_intersection(inputs, targets):\n        intersection = torch.sum(inputs * targets)\n        return intersection\n\n    def forward(self, inputs, target):\n        dice = 0\n        ce = 0\n        CE_L = torch.nn.BCELoss()\n        for i in range(target.size(1)):\n            dice = dice + self.binary_dice(inputs[:, i, ...], target[:, i, ...], i)\n            ce = ce + CE_L(torch.sigmoid(inputs[:, i, ...]), target[:, i, ...])\n        # dice_grad = torch.autograd.grad(dice, inputs, retain_graph=True)[0]\n        # ce_grad = torch.autograd.grad(ce, inputs, retain_graph=True)[0]\n\n        # # Tính toán tỷ lệ gradient\n        # dice_grad_norm = dice_grad.norm(2)\n        # ce_grad_norm = ce_grad.norm(2)\n\n        # # Cập nhật trọng số dựa trên tỷ lệ gradient\n        # gradient_ratio = dice_grad_norm / (ce_grad_norm + 1e-5)\n        # self.weight_dice = min(1.0, max(0.0, self.weight_dice + 0.01 * gradient_ratio))\n        # self.weight_ce = 1.0 - self.weight_dice\n        \n        # # Tính toán final loss với trọng số đã điều chỉnh\n        # final_dice = (self.weight_dice * dice + self.weight_ce * ce) / target.size(1)\n        # return final_dice\n        final_dice = ( 0.75 * dice + 0.25 * ce) / target.size(1)\n        return final_dice\n\n    def metric(self, inputs, target):\n        dices = []\n        for j in range(target.size(0)):\n            dice = []\n            for i in range(target.size(1)):\n                dice.append(self.binary_dice(inputs[j, i], target[j, i], i, True))\n            dices.append(dice)\n        return dices\n\n\nclass EDiceLoss_Val(nn.Module):\n    \"\"\"Dice loss tailored to Brats need.\n    \"\"\"\n\n    def __init__(self, do_sigmoid=True):\n        super(EDiceLoss_Val, self).__init__()\n        self.do_sigmoid = do_sigmoid\n        self.labels = [\"ET\", \"TC\", \"WT\"]\n        self.device = \"cpu\"\n\n    def binary_dice(self, inputs, targets, label_index, metric_mode=False):\n        smooth = 1e-5\n        if self.do_sigmoid:\n            inputs = torch.sigmoid(inputs)\n\n        if metric_mode:\n            inputs = inputs > 0.5\n            if targets.sum() == 0:\n                print(f\"No {self.labels[label_index]} for this patient\")\n                if inputs.sum() == 0:\n                    return torch.tensor(1., device=\"cuda\")\n                else:\n                    return torch.tensor(0., device=\"cuda\")\n            # Threshold the pred\n        intersection = EDiceLoss_Val.compute_intersection(inputs, targets)\n        if metric_mode:\n            dice = (2 * intersection) / ((inputs.sum() + targets.sum()) * 1.0)\n        else:\n            dice = (2 * intersection + smooth) / (inputs.pow(2).sum() + targets.pow(2).sum() + smooth)\n        if metric_mode:\n            return dice\n        return 1 - dice\n\n    @staticmethod\n    def compute_intersection(inputs, targets):\n        intersection = torch.sum(inputs * targets)\n        return intersection\n\n    def forward(self, inputs, target):\n        dice = 0\n        for i in range(target.size(1)):\n            dice = dice + self.binary_dice(inputs[:, i, ...], target[:, i, ...], i)\n        final_dice = dice / target.size(1)\n        return final_dice\n\n    def metric(self, inputs, target):\n        dices = []\n        for j in range(target.size(0)):\n            dice = []\n            for i in range(target.size(1)):\n                dice.append(self.binary_dice(inputs[j, i], target[j, i], i, True))\n            dices.append(dice)\n        return dices\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\"\"\"\n\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    @staticmethod\n    def _get_batch_fmtstr(num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:54.041474Z","iopub.execute_input":"2025-10-20T14:28:54.041856Z","iopub.status.idle":"2025-10-20T14:28:54.062201Z","shell.execute_reply.started":"2025-10-20T14:28:54.041815Z","shell.execute_reply":"2025-10-20T14:28:54.061270Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Traing","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npost_trans = Compose(\n    [EnsureType(), Activations(sigmoid=True), AsDiscrete(argmax=False, threshold=0.5)]\n)\n\npost_sigmoid = Activations(sigmoid=True)\npost_pred = AsDiscrete(argmax=False, threshold=0.5)\nroi = (128, 128, 128) #128, 128, 128\nsw_batch_size = 1\noverlap = 0.5\nVAL_AMP = True\n\nwandb.login(key = \"\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:28:54.063179Z","iopub.execute_input":"2025-10-20T14:28:54.063501Z","iopub.status.idle":"2025-10-20T14:28:54.298944Z","shell.execute_reply.started":"2025-10-20T14:28:54.063475Z","shell.execute_reply":"2025-10-20T14:28:54.298027Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_post_trans(tta=False):\n    if tta:\n        return Compose([\n            EnsureType(),\n            AsDiscrete(argmax=False, threshold=0.5)\n        ])\n    return Compose([\n        EnsureType(),\n        Activations(sigmoid=True),\n        AsDiscrete(argmax=False, threshold=0.5)\n    ])\n\n#----------------\ndef train_epoch(model, loader, optimizer, epoch, loss_func, end, wandb_tracking = 0):\n    model.train()\n    run_loss = AverageMeter('Loss', ':.4e')\n\n    num_steps = len(loader)\n    print(f\"Epoch {epoch}: Number of steps (batches) in this epoch: {num_steps}\")\n\n    for idx, batch_data in enumerate(loader):\n        torch.cuda.empty_cache()\n        data, target = batch_data[\"image\"].float().cuda(), batch_data[\"label\"].float().cuda()\n        logits = model(data)\n\n        loss = loss_func(logits, target)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        run_loss.update(loss.item(), n=batch_size)\n        if wandb_tracking == 1:\n            wandb.log({\"train_step_loss\": loss.item()})\n\n    # Log the average loss for the epoch\n    if wandb_tracking == 1:\n        wandb.log({\"train_epoch_loss\": run_loss.avg, \"epoch\": epoch})\n    return run_loss.avg\n\n# ===============================================================================================\ndef model_inferer(input, model, tta=True, batch_size=2, overlap=0.6):\n    def _compute(input, flip_dims=None):\n        inputs = input if flip_dims is None else input.flip(dims=flip_dims)\n        with torch.amp.autocast(device_type='cuda', enabled=VAL_AMP):\n            output = sliding_window_inference(\n                inputs=inputs,\n                roi_size=roi,\n                sw_batch_size=batch_size,\n                predictor=model,\n                overlap=overlap,\n            )\n        if flip_dims is not None:\n            output = output.flip(dims=flip_dims)\n        return output\n\n    with torch.no_grad():\n        if tta:\n            # Danh sách các phép flip\n            flip_combinations = [\n                None,  # Gốc\n                (2,),  # Flip x\n                (3,),  # Flip y\n                (4,),  # Flip z\n                (2, 3),  # Flip x, y\n                (2, 4),  # Flip x, z\n                (3, 4),  # Flip y, z\n                (2, 3, 4),  # Flip x, y, z\n            ]\n            predict = None\n            for flip_dims in flip_combinations:\n                output = _compute(input, flip_dims)\n                output = torch.sigmoid(output)  # Chuyển logits thành xác suất\n                predict = output if predict is None else predict + output\n            predict = predict / 8.0  # Trung bình\n        else:\n            predict = _compute(input)  # Inference gốc, trả về logits\n\n    torch.cuda.empty_cache()\n    return predict\n\n\n# ===============================================================================================\n\ndef evaluate_model(model, loader, epoch, acc_func, criterian_val, metric, wandb_tracking=0, tta=False):\n    model.eval()\n    run_acc = AverageMeter('Loss', ':.4e')\n    all_preds = []\n    all_labels = []\n    post_trans = get_post_trans(tta)  # Chọn post_trans dựa trên tta\n    with torch.no_grad():\n        for idx, batch_data in enumerate(loader):\n            val_inputs, val_labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n\n            logits = model_inferer(val_inputs, model, tta=tta, batch_size=2, overlap=0.6)\n            val_outputs_list = decollate_batch(logits)\n            val_labels_list = decollate_batch(val_labels)\n\n            # Áp dụng post_trans\n            val_output_convert = [post_trans(val_pred_tensor).to(device) for val_pred_tensor in val_outputs_list]\n            val_labels_convert = [label.to(device) for label in val_labels_list]\n            del val_inputs, val_labels, logits, val_outputs_list, val_labels_list\n            torch.cuda.empty_cache()\n            all_preds.extend(val_output_convert)\n            all_labels.extend(val_labels_convert)\n\n            # Tính metrics\n            acc_func.reset()\n            acc_func(y_pred=val_output_convert, y=val_labels_convert)\n            acc, not_nans = acc_func.aggregate()\n            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n\n        dice_et = run_acc.avg[0]\n        dice_tc = run_acc.avg[1]\n        dice_wt = run_acc.avg[2]\n\n    # Log validation metrics\n    if wandb_tracking == 1:\n        wandb.log({\n            \"val_epoch_dice_et\": dice_et,\n            \"val_epoch_dice_tc\": dice_tc,\n            \"val_epoch_dice_wt\": dice_wt,\n            \"epoch\": epoch\n        })\n    return run_acc.avg\n\n# ===============================================================================================\n\n\ndef trainer(model, train_loader, val_loader, optimizer, loss_func, \n            acc_func, criterian_val, metric, scheduler, tta=True, start_epoch=1, end_epoch=3, save_every=1, checkpoint_path=None, wandb_tracking = 0):\n    # Initialize wandb logging\n    if wandb_tracking:\n        wandb.init(entity=\"uit-meow\", project=\"medical\", name=\"test-med1\", config={\n            \"epochs\": end_epoch,\n            \"optimizer\": optimizer.__class__.__name__,\n            \"learning_rate\": optimizer.param_groups[0][\"lr\"],\n            \"loss_func\": loss_func.__class__.__name__,\n            \"scheduler\": scheduler.__class__.__name__ if scheduler else None\n        })\n        wandb_table = wandb.Table(columns=[\"Epoch\", \"Train Loss\", \"Dice_ET\", \"Dice_TC\", \"Dice_WT\", \"val_avg_acc\", \"Best Model\"])\n\n    val_acc_max = 0.0\n    best_epoch = 0\n    TC_dices = []\n    WT_dices = []\n    ET_dices = []\n    avg_dices = []\n    loss_epochs, train_epochs = [], []\n\n    # If checkpoint path is provided, load model and optimizer states\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, weights_only=False)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        if scheduler:\n            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch\n        val_acc_max = checkpoint['val_acc_max']  # Retain the best validation accuracy\n\n        print(f\"Resuming training from epoch {start_epoch}...\")\n    \n    for epoch in range(start_epoch, end_epoch + 1):\n        # Train the model for the current epoch\n        train_loss = train_epoch(model, train_loader, optimizer, epoch=epoch, loss_func=loss_func, end=end_epoch, wandb_tracking = wandb_tracking)\n        torch.cuda.empty_cache()\n\n        # Update scheduler if available\n        if scheduler is not None:\n            scheduler.step()\n        \n        # Evaluate the model after every 'save_every' epoch or at the last epoch\n        if epoch % save_every == 0 or epoch == end_epoch or epoch == start_epoch:\n            loss_epochs.append(train_loss)\n            train_epochs.append(epoch)\n            val_acc = evaluate_model(model, val_loader, epoch=epoch,\n                                    acc_func=acc_func, criterian_val=criterian_val, \n                                    metric=metric, wandb_tracking=wandb_tracking, tta=tta)\n            ET_dice = val_acc[0]\n            TC_dice = val_acc[1]\n            WT_dice = val_acc[2]\n            val_avg_acc = np.mean(val_acc)\n\n            # Update dice coefficients\n            ET_dices.append(ET_dice)\n            TC_dices.append(TC_dice)\n            WT_dices.append(WT_dice)\n            avg_dices.append(val_avg_acc)\n\n            # Check if this is the best model\n            best_model_flag = False\n            if val_avg_acc > val_acc_max:\n                print(f\"New best ({val_acc_max:.6f} --> {val_avg_acc:.6f}) at epoch {epoch}\")\n                val_acc_max = val_avg_acc\n                best_epoch = epoch\n                best_model_flag = True\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),  # Save scheduler state\n                'val_acc_max': val_acc_max,\n            }, save_current)  # Save model to 'model.pth'\n            wandb_table.add_data(epoch, train_loss, ET_dice, TC_dice, WT_dice, val_avg_acc, \"Yes\" if best_model_flag else \"No\")\n            if best_model_flag:\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),  # Save scheduler state\n                    'val_acc_max': val_acc_max,\n                }, save_path)  # Save model to 'model.pth'\n                \n\n            torch.cuda.empty_cache()\n\n                \n\n    # ✅ Log table to wandb\n    if wandb_tracking == 1:\n        wandb.log({\"Training Metrics\": wandb_table})\n\n    return (val_acc_max, TC_dices, WT_dices, ET_dices, avg_dices, loss_epochs, train_epochs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:45:57.752465Z","iopub.execute_input":"2025-10-20T14:45:57.752957Z","iopub.status.idle":"2025-10-20T14:45:57.778888Z","shell.execute_reply.started":"2025-10-20T14:45:57.752924Z","shell.execute_reply":"2025-10-20T14:45:57.778163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"start = 0\nend = 200#max_epochs\nbatch_size = 1\nsave_every = 2 #\n\nlearning_rate = 3e-4\nweight_decay = 1e-5\ncheckpoint_path = \"/kaggle/input/again21/pytorch/default/1/best_metric_model(1).pth\"#/kaggle/input/scsenewsotalap/pytorch/default/1/scsenew_sotaLap.pth\"\n###\nsave_dir = \"/kaggle/working\"\nos.makedirs(save_dir, exist_ok=True)\nsave_path = os.path.join(save_dir, \"best_metric_model.pth\")\nsave_current = os.path.join(save_dir, \"current_model.pth\")\n###\n\ncriterion = EDiceLoss().cuda()\ncriterian_val = EDiceLoss_Val().cuda()\nmetric = criterian_val.metric\n\ndice_acc = DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=True)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = end)\n\n\n(val_acc_max, TC_dices, WT_dices, ET_dices, avg_dices, loss_epochs, train_epochs)  = trainer(model = model,\n                                                                                            train_loader = train_loader,\n                                                                                            val_loader = val_loader,\n                                                                                            optimizer = optimizer,\n                                                                                            loss_func = criterion,\n                                                                                            acc_func = dice_acc,\n                                                                                            criterian_val = criterian_val,\n                                                                                            metric = metric,\n                                                                                            scheduler = scheduler,\n                                                                                            start_epoch = start,\n                                                                                            end_epoch = end,\n                                                                                            save_every = save_every,\n                                                                                            checkpoint_path = checkpoint_path,\n                                                                                            wandb_tracking = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:46:00.061079Z","iopub.execute_input":"2025-10-20T14:46:00.061798Z","iopub.status.idle":"2025-10-20T14:46:00.070217Z","shell.execute_reply.started":"2025-10-20T14:46:00.061760Z","shell.execute_reply":"2025-10-20T14:46:00.069256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom monai.metrics import DiceMetric\nfrom monai.metrics import HausdorffDistanceMetric\nfrom medpy.metric import binary\nfrom monai.data import decollate_batch\nfrom monai.inferers import sliding_window_inference\nfrom monai.transforms import (\n    Activations,\n    AsDiscrete,\n    Compose,\n    EnsureType\n)\n\ncheckpoint = torch.load(checkpoint_path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\ndef val_epoch_hd95(model, loader, max_epochs, epoch, acc_func, metric, device):\n    model.eval()\n    run_acc = AverageMeter('Loss', ':.4e')\n    run_acc1 = AverageMeter('Loss', ':.4e')\n    hausdorff_metric = HausdorffDistanceMetric(include_background=True, reduction='mean_batch', get_not_nans=True)\n    hd_metric = []\n    hd95_metric = []\n    with torch.no_grad():\n        for idx, batch_data in enumerate(loader):\n            val_inputs, val_labels = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n            logits = model_inferer(val_inputs, model)\n\n            val_outputs_list = decollate_batch(logits)\n            val_labels_list = decollate_batch(val_labels)\n\n            val_output_convert = [post_trans(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n            acc_func.reset()\n            acc_func(y_pred=val_output_convert, y=val_labels_list)\n\n\n            hausdorff_metric.reset()\n            hausdorff_metric(y_pred=val_output_convert, y=val_labels_list)\n            acc1, not_nans1 = hausdorff_metric.aggregate()\n            run_acc1.update(acc1.cpu().numpy(), n=not_nans1.cpu().numpy())\n            hausdorff_et = run_acc1.avg[0]\n            hausdorff_tc = run_acc1.avg[1]\n            hausdorff_wt = run_acc1.avg[2]\n\n            segs = logits\n            targets = val_labels\n            hd = []\n            hd95 = []\n            dice = []\n            for l in range(segs.shape[1]):\n                if targets[0,l].cpu().numpy().sum() == 0:\n                    hd.append(1)\n                    hd95.append(0)\n                    print((segs[0,l].cpu().numpy() > 0.5).sum())\n\n                    continue\n                if (segs[0,l].cpu().numpy() > 0.5).sum() == 0:\n                    hd.append(0)\n                    hd95.append(0)\n                    continue\n\n                hd.append(binary.hd(segs[0,l].cpu().numpy() > 0.5, targets[0,l].cpu().numpy() > 0.5, voxelspacing=None))\n\n                hd95.append(binary.hd95(segs[0,l].cpu().numpy() > 0.5, targets[0,l].cpu().numpy() > 0.5, voxelspacing=None))\n            hd_metric.append(hd)\n            hd95_metric.append(hd95)\n\n\n\n            hd_metric_mean = [np.nanmean(l) for l in zip(*hd_metric)]\n            hd95_metric_mean = [np.nanmean(l) for l in zip(*hd95_metric)]\n            acc, not_nans = acc_func.aggregate()\n            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n            dice_et = run_acc.avg[0]\n            dice_tc = run_acc.avg[1]\n            dice_wt = run_acc.avg[2]\n\n\n\n    labels = (\"ET\", \"TC\", \"WT\")\n    #metrics = {key: value for key, value in zip(labels, metrics)}\n\n    hd_metric = [np.nanmean(l) for l in zip(*hd_metric)]\n    hd95_metric = [np.nanmean(l) for l in zip(*hd95_metric)]\n    print(\"hd_metric_final: \", hd_metric)\n    print(\"hd95_metric_final: \", hd95_metric)\n\n    return run_acc.avg\n\ndef calc_hd95(model, val_loader, device, max_epochs):\n    dice_acc = DiceMetric(include_background=True, reduction='mean_batch', get_not_nans=True)\n    criterian_val = EDiceLoss_Val().to(device)\n    metric = criterian_val.metric\n\n    val_acc = val_epoch_hd95(model, val_loader, max_epochs, epoch=0, acc_func=dice_acc, metric = metric, device=device)\n    dice_et, dice_tc, dice_wt = val_acc[0], val_acc[1], val_acc[2]\n    val_avg_acc = np.mean(val_acc)\n    print(f\"\\t{'*' * 20}Epoch Summary{'*' * 20}\")\n    print(f\"Final validation stats {1}/{max_epochs}, dice_et: {dice_et:.6f}, dice_tc: {dice_tc:.6f}, dice_wt: {dice_wt:.6f} , Dice_Avg: {val_avg_acc:.6f}\")\n\n\ncalc_hd95(model, val_loader, device, end)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:46:06.783847Z","iopub.execute_input":"2025-10-20T14:46:06.784338Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs_data = [\n    {\n        \"Epoch\": train_epochs[i],\n        \"TC_dices\": TC_dices[i] if i < len(TC_dices) else None,\n        \"WT_dices\": WT_dices[i] if i < len(WT_dices) else None,\n        \"ET_dices\": ET_dices[i] if i < len(ET_dices) else None,\n        \"avg_dices\": avg_dices[i] if i < len(avg_dices) else None,\n        \"loss_epochs\": loss_epochs[i] if i < len(loss_epochs) else None,\n    }\n    for i in range(len(train_epochs))\n]\n\nimport pandas as pd\n\nepochs_df = pd.DataFrame(epochs_data)\nprint(epochs_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-20T14:43:23.389201Z","iopub.execute_input":"2025-10-20T14:43:23.389643Z","iopub.status.idle":"2025-10-20T14:43:23.457829Z","shell.execute_reply.started":"2025-10-20T14:43:23.389592Z","shell.execute_reply":"2025-10-20T14:43:23.456660Z"}},"outputs":[],"execution_count":null}]}